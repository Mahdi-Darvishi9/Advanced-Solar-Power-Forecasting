{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671614e7",
   "metadata": {},
   "source": [
    "# Advanced Solar Power Forecasting\n",
    "This notebook trains and evaluates a hybrid ensemble model combining **LightGBM** and **XGBoost**, optimized through both Grid Search and Bayesian Search, for predicting solar power generation. Note: only a part of the code is available publicly.\n",
    "The goal is to achieve high accuracy using only weather and location-based features — without solar irradiance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df5c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost lightgbm scikit-optimize\n",
    "\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error, explained_variance_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from itertools import combinations\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee6858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "# Assuming 'df' is the main DataFrame and 'PolyPwr' is the target variable\n",
    "X = df.drop(['PolyPwr'], axis=1)\n",
    "y = df['PolyPwr']\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.176, random_state=42)  # ≈ 15% of 85%\n",
    "\n",
    "# Drop non-feature columns\n",
    "X_train = X_train.drop(columns=['Location'])\n",
    "X_val = X_val.drop(columns=['Location'])\n",
    "X_test = X_test.drop(columns=['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfa3abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Models (LightGBM and XGBoost)\n",
    "# Train baseline LightGBM and XGBoost models\n",
    "lightgbm_model = LGBMRegressor(random_state=42)\n",
    "xgboost_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "lightgbm_model.fit(X_train, y_train)\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Validation predictions\n",
    "val_predictions_lgbm = lightgbm_model.predict(X_val)\n",
    "val_predictions_xgb = xgboost_model.predict(X_val)\n",
    "\n",
    "# Test predictions\n",
    "test_predictions_lgbm = lightgbm_model.predict(X_test)\n",
    "test_predictions_xgb = xgboost_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dfcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Meta-Model\n",
    "# Combine model predictions and train a Linear Regression meta-model\n",
    "stacked_val_predictions = np.column_stack((val_predictions_lgbm, val_predictions_xgb))\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_val_predictions, y_val)\n",
    "\n",
    "# Predict on test data\n",
    "stacked_test_predictions = np.column_stack((test_predictions_lgbm, test_predictions_xgb))\n",
    "final_predictions = meta_model.predict(stacked_test_predictions)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, final_predictions)\n",
    "print(f\"Stacked Ensemble Model MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4b177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (Grid Search)\n",
    "param_grid_lgbm = {\n",
    "    'num_leaves': [31, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "grid_search_lgbm = GridSearchCV(estimator=lgbm, param_grid=param_grid_lgbm, cv=3,\n",
    "                                scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search_lgbm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best LightGBM parameters:\", grid_search_lgbm.best_params_)\n",
    "print(\"Best RMSE:\", (-grid_search_lgbm.best_score_)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (XGBoost)\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_reg, param_grid=param_grid_xgb, cv=3,\n",
    "                               scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", grid_search_xgb.best_params_)\n",
    "print(\"Best RMSE:\", (-grid_search_xgb.best_score_)**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc59d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized Models and Evaluation\n",
    "# Retrain optimized models\n",
    "lightgbm_model_opt = LGBMRegressor(learning_rate=0.05, max_depth=7, n_estimators=300, num_leaves=31, random_state=42)\n",
    "xgboost_model_opt = xgb.XGBRegressor(colsample_bytree=1.0, learning_rate=0.05, max_depth=7, n_estimators=200,\n",
    "                                     subsample=0.8, random_state=42)\n",
    "\n",
    "lightgbm_model_opt.fit(X_train, y_train)\n",
    "xgboost_model_opt.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble averaging\n",
    "preds_lgbm = lightgbm_model_opt.predict(X_test)\n",
    "preds_xgb = xgboost_model_opt.predict(X_test)\n",
    "ensemble_preds = (preds_lgbm + preds_xgb) / 2\n",
    "\n",
    "# Evaluate ensemble\n",
    "mse = mean_squared_error(y_test, ensemble_preds)\n",
    "r2 = r2_score(y_test, ensemble_preds)\n",
    "print(f\"Optimized Ensemble MSE: {mse}\")\n",
    "print(f\"Optimized Ensemble R²: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92243262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Validation (Model Stability)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "r2_scorer = make_scorer(r2_score)\n",
    "\n",
    "# Cross-validation for LightGBM\n",
    "lgbm_cv_r2 = cross_val_score(lightgbm_model_opt, X, y, cv=tscv, scoring=r2_scorer)\n",
    "print(f\"LightGBM CV R² Scores: {lgbm_cv_r2}\")\n",
    "print(f\"Average: {np.mean(lgbm_cv_r2):.4f}\")\n",
    "\n",
    "# Cross-validation for XGBoost\n",
    "xgb_cv_r2 = cross_val_score(xgboost_model_opt, X, y, cv=tscv, scoring='r2')\n",
    "print(f\"XGBoost CV R² Scores: {xgb_cv_r2}\")\n",
    "print(f\"Average: {np.mean(xgb_cv_r2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d254ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering (Polynomial + Interactions)\n",
    "# Add interaction and polynomial features\n",
    "feature_names = ['AmbientTemp', 'Humidity']\n",
    "\n",
    "def add_interaction_terms(df, feature_names):\n",
    "    combos = combinations(feature_names, 2)\n",
    "    for combo in combos:\n",
    "        new_feature_name = 'x'.join(combo)\n",
    "        df[new_feature_name] = df[combo[0]] * df[combo[1]]\n",
    "    for feature_name in feature_names:\n",
    "        df[f\"{feature_name}^2\"] = df[feature_name] ** 2\n",
    "    return df\n",
    "\n",
    "df = add_interaction_terms(df, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c420a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian Optimization (LightGBM)\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(0.01, 0.3, 'log-uniform'),\n",
    "    'n_estimators': Integer(100, 1000),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'num_leaves': Integer(20, 150),\n",
    "    'colsample_bytree': Real(0.6, 1.0, 'uniform'),\n",
    "    'subsample': Real(0.6, 1.0, 'uniform'),\n",
    "}\n",
    "\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "opt = BayesSearchCV(lgbm, search_spaces, scoring=make_scorer(mean_squared_error, greater_is_better=False),\n",
    "                    n_iter=32, cv=3, n_jobs=-1, random_state=42)\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", opt.best_params_)\n",
    "print(\"Best MSE:\", -opt.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec460ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Ensemble Meta-Model Evaluation\n",
    "# Final stacking meta-model using optimized LightGBM & XGBoost\n",
    "predictions_lgbm = lightgbm_model_opt.predict(X_test)\n",
    "predictions_xgb = xgboost_model_opt.predict(X_test)\n",
    "stacked_predictions = np.column_stack((predictions_lgbm, predictions_xgb))\n",
    "\n",
    "meta_model = LinearRegression()\n",
    "meta_model.fit(stacked_predictions, y_test)\n",
    "final_predictions = meta_model.predict(stacked_predictions)\n",
    "\n",
    "# Metrics\n",
    "meta_mse = mean_squared_error(y_test, final_predictions)\n",
    "meta_r2 = r2_score(y_test, final_predictions)\n",
    "mae = mean_absolute_error(y_test, final_predictions)\n",
    "rmse = mean_squared_error(y_test, final_predictions, squared=False)\n",
    "explained_var = explained_variance_score(y_test, final_predictions)\n",
    "\n",
    "print(f\"MSE: {meta_mse}\")\n",
    "print(f\"R²: {meta_r2}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"Explained Variance: {explained_var}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
